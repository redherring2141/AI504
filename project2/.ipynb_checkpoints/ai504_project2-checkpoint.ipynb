{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owh7m9N7baNm"
   },
   "source": [
    "# AI504 Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nLERzWfbaNm"
   },
   "source": [
    "## To-Do : Find better hyperparameters\n",
    "The goal of this project is improving the performance of Neural Machine Translation(NMT) system. In this project, you will tune the hyperparameters to achieve higher BLEU score without changing anything else (e.g. architecture, dataset, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnZg7YImbaNm"
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "config = EasyDict({\n",
    "    \"emb_dim\":64,\n",
    "    \"ffn_dim\":128,\n",
    "    \"attention_heads\":8,\n",
    "    \"dropout\":0.2518,\n",
    "    \"encoder_layers\":3,\n",
    "    \"decoder_layers\":2,\n",
    "    \"lr\":0.0007894,\n",
    "    \"batch_size\":461,\n",
    "    \"nepochs\":48,\n",
    "})\n",
    "\n",
    "#####      Do not modify      #####\n",
    "config.max_position_embeddings=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcCbVaGobaNm"
   },
   "source": [
    "## Download files\n",
    "Before execute this code, you should run the template codes first. This code will automatically downloads the state_dict of your model and configuration file which you use for training & evaluation.\n",
    "\n",
    "Please change the student ID before you run this.\n",
    "\n",
    "__CAUTION__ : Please run this code with *Google Chrome* browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85HLP7FFbaNm"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.environ['STUDENT_ID']=\"20201234\"\n",
    "\n",
    "if os.path.isdir('result'):\n",
    "  !rm -rf result\n",
    "\n",
    "%mkdir result\n",
    "%mv config.json model.pt result\n",
    "\n",
    "!zip $STUDENT_ID.zip result/*\n",
    "files.download('{}.zip'.format(os.environ['STUDENT_ID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lxUcbecbaNm"
   },
   "source": [
    "## Template codes (do not modify)\n",
    "This code is equivalent to the code in [Week 11](https://classum.com/main/course/7726/111). Please refer to codes & descriptions in a link for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5ng6B23baNm"
   },
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXtGXREvbaNm"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torchtext\n",
    "!python -m spacy download de\n",
    "!python -m spacy download en\n",
    "!pip install -Iv --upgrade nltk==3.5\n",
    "\n",
    "import torch\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 3)\n",
    "TRG.build_vocab(train_data, min_freq = 3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = config.batch_size,\n",
    "    device = device,\n",
    "    shuffle=False)\n",
    "\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JqEbcoKbaNm"
   },
   "source": [
    "### Load model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyj44zHjbaNm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(len(SRC.vocab),config.emb_dim)\n",
    "        self.decoder_embedding = nn.Embedding(len(TRG.vocab),config.emb_dim)\n",
    "        self.transformer = nn.Transformer(d_model=config.emb_dim, nhead=config.attention_heads, \n",
    "                       num_encoder_layers=config.encoder_layers, num_decoder_layers=config.decoder_layers,\n",
    "                       dim_feedforward=config.ffn_dim, dropout=config.dropout, activation='gelu')\n",
    "        self.prediction_head = nn.Linear(config.emb_dim,len(TRG.vocab))\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        src_emb = self.encoder_embedding(src)\n",
    "        trg_emb = self.decoder_embedding(trg)\n",
    "        output = self.transformer(src_emb, trg_emb,\n",
    "                       tgt_mask=self.transformer.generate_square_subsequent_mask(trg.size(0)).to(device),\n",
    "                       src_key_padding_mask=src.eq(PAD_IDX).permute(1,0).to(device),\n",
    "                       memory_key_padding_mask=src.eq(PAD_IDX).permute(1,0).to(device),\n",
    "                       tgt_key_padding_mask=trg.eq(PAD_IDX).permute(1,0).to(device))\n",
    "        prediction = self.prediction_head(output)\n",
    "        return prediction\n",
    "\n",
    "CLIP = 1\n",
    "    \n",
    "model = Transformer(config)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIyuC73ObaNm"
   },
   "source": [
    "### Train & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3amnpTLbaNm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[:-1].reshape(-1, output.shape[-1])\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src, trg)            \n",
    "            \n",
    "            output = output[:-1].reshape(-1, output.shape[-1])\n",
    "            \n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def measure_BLEU(model: nn.Module,\n",
    "             iterator: BucketIterator\n",
    "                ):\n",
    "    model.eval()\n",
    "    iterator.batch_size = 1\n",
    "    BLEU_scores = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src, trg)           \n",
    "            predicted = [TRG.vocab.itos[token] for token in output[:-1].argmax(dim=2).squeeze().tolist() if token!=PAD_IDX]\n",
    "            GT = [TRG.vocab.itos[token] for token in trg[1:].squeeze().tolist() if token!=PAD_IDX]\n",
    "            BLEU_scores.append(sentence_bleu([GT], predicted))\n",
    "    return sum(BLEU_scores)/len(BLEU_scores)\n",
    "                         \n",
    "queue=0\n",
    "for epoch in tqdm(range(config.nepochs), total=config.nepochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    test_bleu = measure_BLEU(model, test_iterator)\n",
    "    print(\"Test BLEU score : {}\".format(test_bleu))\n",
    "    print(\"Epoch : {} / Training loss : {} / Validation loss : {}\".format(epoch+1, train_loss, valid_loss))\n",
    "\n",
    "    if best_valid_loss < valid_loss:\n",
    "        queue+=1\n",
    "        if queue>1:\n",
    "            break\n",
    "    else:\n",
    "        best_valid_loss = valid_loss\n",
    "        queue = 0\n",
    "\n",
    "test_bleu = measure_BLEU(model, test_iterator)\n",
    "print(\"Test BLEU score : {}\".format(test_bleu))\n",
    "        \n",
    "with open('config.json','w') as f:\n",
    "    json.dump(vars(config),f)\n",
    "torch.save(model.state_dict(),'model.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
