10:40:58	 From 이성준 : loss를 줄이려는 학습의 결과로 변한 분포를 왜 다시 맞춰줘야 하는지 궁금합니다.
10:43:37	 From 박성진 20204341 : wondering in what cases batchnorm can have a bad effect. (like viewpoint of data distribution or feature characteristics)
10:43:42	 From 20203210_Jiyoon_Myung : Does BN occur between input layer and first hidden layer also?
10:44:50	 From 20185051 SoJung : mu랑 sigma도 k layer를 학습하기 위한 input으로 들어가나요? gamma랑 beta만 들어가나요?
10:45:43	 From jaegyunkim : If I don’t mis-remembered, last class, prof said, in these day, we don’t use BN. Why?
10:45:58	 From 20185051 SoJung : 아, gamma랑 beta가 그 역할을 해주는거네요.
10:46:12	 From 이성준 : 네, 감사합니다.
10:50:59	 From jaegyunkim : batch 사이즈가 작으면 batch normalize 하는 것도 bias가 크겠네요?
10:54:18	 From jaegyunkim : 아 계속해서 keeping하고 업데이트하니깐 괜찮군요.
11:02:27	 From M_20194421_백유미 : C, N이 survive한다는 의미가 무엇인가요?
11:07:22	 From __ : ^ C,N dimension 에 대해 normalize되지 않는다는 것 같습니다
11:09:37	 From M_20194421_백유미 : 특정 C, N을 이렇게 survive하게 만드는 이유가 무엇인가요? 특정한 목적을 위해서 이렇게 구별된 것 같다고 생각되어서요.
11:15:30	 From 20204315_나지혜 : yes
11:15:31	 From jaegyunkim : yes
11:15:31	 From Haneul Yoo : yes
11:15:31	 From 박성진 20204341 : yes
11:15:32	 From user : 넵.
11:15:32	 From Andrés Brito : yes
11:24:08	 From M_20194421_백유미 : When use the input dimension like (n>=2, 3, 32, 32)?  
11:25:23	 From WonhongYoo : Does the 'RandomCrop' work during test time?
11:25:27	 From 20203491 이재완 : transforms.Normalize에 들어가는 값들은 어떻게 정해진건가요?
11:26:44	 From soro bedionita : is centercrop the same as randomcrop?
11:27:07	 From Rushda Basir : Why is RandomHorizontal Flip needed?
11:27:51	 From 20203210_Jiyoon_Myung : transform은 denoised autoencoder와 비슷한 원리인가요?
11:34:10	 From Yoonjae Choi : @WonhongYoo: RandomCrop is only used during the training phase, in order to obtain a more robust classifier.
11:35:33	 From Yoonjae Choi : @이재완: The values given to transforms.Normalize are the mean and standard deviations of each R, G, B channel.
11:35:58	 From Yoonjae Choi : @Soro: No, CenterCrop and RandomCrop operate differently.
11:36:30	 From 이성준 : 어느부분이 residual connection 인가요?
11:37:07	 From Yoonjae Choi : @Rushda, RandomHorizontalFlip is used to train the model with left-right flipped images from time to time. This will have the effect of doubling the size of the training data, and will make your model more robust.
11:37:20	 From 이성준 : 코드
11:37:32	 From 이성준 : 아래부분 보여주시기 전에 질문입니다
11:39:27	 From jaegyunkim : residual을 이용하는건,
불필요한 ConvNet 학습을 줄이기 위함 아닌가요?
out에 shortcut을 선택하던 convnet을 이용하건 해야하는거 아닌가요?
11:41:19	 From 박영서 : down sampling 역할은 무엇인가요?
11:42:58	 From soro bedionita : I do not get why this statement if stride==2 down_sample el else down_sample
11:43:38	 From soro bedionita : we downsample no matter the condition
11:43:58	 From 곽영준 : Fisrt Conv of Resnet why it has big kernal size as 7 x 7. It is for downsize of the input or some other reason i wonder...
11:44:43	 From Yoonjae Choi : out =. Batshortcut + out이
11:46:52	 From Yoonjae Choi : @jaegyunkim: out = shortcut + out이 되어야지 이전 layer의 아웃풋이 다음 layer에 전달되고, 현재 layer는 그 사이의 변화량만 배울 수 있게 됩니다. shortcut과 out 중 하나를 선택하게 된다면 우리가 원하는 바를 달성할 수 없습니다.
11:47:58	 From jaegyunkim : 그렇군요! 감사합니다.
11:49:22	 From 곽영준 : Before first of Resblock, there is a conv and pool layer at begining.. why that is needed??
11:49:36	 From Rushda Basir : Is downsampling used for each residual block?
11:49:38	 From Yoonjae Choi : @곽영준: I think your assumption is correct, that they use 7x7 filter with stride=2 in the very first step to significantly downsize the input.
11:49:55	 From 곽영준 : I see. Thahk U
11:50:55	 From 박영서 : stride는 임의로 설정하는 것이 아니라 size 맞춰주는 것에 따라 바뀌는 것인가요?
11:51:01	 From Rushda Basir : Thankyou
11:55:40	 From 20185051 SoJung : 갑자기 감소하는건 특별한 현상이 일어났기 때문인가요, 아니면 그냥 우연인가요?
11:57:13	 From 20185051 SoJung : 아..! 감사합니다!
11:58:13	 From 주지환 : 감사합니다.
11:59:53	 From Hyungyung Lee : 화면이 안보이는데, 저만 그런건가요…??
12:00:09	 From ______ : could TA upload the solutions by this afternoon?
12:00:18	 From ______ : Thank you!
12:05:22	 From 20209007 최상범 : Not actually
12:05:31	 From Rushda Basir : if you could explain it would be great
12:05:38	 From Rushda Basir : me
12:05:40	 From 20185051 SoJung : 설명 해주셨으면 좋겠습니다
12:06:35	 From Andrés Brito : I still have a question about the resnet. I do not understand why we skip the layers?
12:06:58	 From Andrés Brito : if we are not using them, why do we keep?
12:09:48	 From 20209007 최상범 : To maintain the signal strength of error back propagation?
12:11:20	 From 박영서 : 그럼 network는 input과 output의 차이를 학습하는 건가요?
12:11:27	 From Andrés Brito : Ok, thanks
12:12:14	 From 이성준 : 배울것이 없을 때 F=0로 만들게 된다면 모든 레이어에 residual 연결을 두어도 같은 결과를 얻을 수 있는 것인가요?
12:12:41	 From 이성준 : 배울것이 있을 때는 F의 영향이 크게 만드는
12:12:49	 From Rushda Basir : during back propagation it transcends through the skipped layers as well? 
12:13:01	 From user : skip connection사이의 network은 0이 되도록 학습이 되는거면 일부 node를 disable하는 dropout과 비슷한 효과를 내는건가요?
12:13:34	 From 박영서 : identity mapping을 하면 왜 optimization problem이 해결되는 건가요? shallower model에 identity mapping으로 deeper model을 구성하는건가요?
12:16:18	 From 이성준 : 왜 몇개 층에 한번씩 두는지?
12:16:35	 From 이성준 : 네, 감사합니다.
12:17:12	 From Dannyel Dani : Professor, would you please explain why some people do flattening before dropout and then they dense the layer?
12:17:46	 From user : 감사합니다
12:18:45	 From 박영서 : 감사합니다.
12:19:14	 From Dannyel Dani : nn.flatten()
12:20:48	 From Dannyel Dani : thanks
12:34:56	 From 20185051 SoJung : C'은 동일한건가요?
12:37:08	 From 20185051 SoJung : 중간중간 빠져나온 것은 back propagation에 활용된다고 이해했는데, 맞나요?
12:37:15	 From 20203491 이재완 : 최종 classifier랑 intermediate classifier의 구성은 같나요?
12:37:20	 From 20185051 SoJung : 감사합니다
12:40:48	 From 20203491 이재완 : 교수님 batch normalization 관련해서 혼동되서 그런데, 매 layer마다 batch normalization을 사용하면 모든 layer들의 input distribution이 모두 유사해지는게 맞나요?
12:40:58	 From Rushda Basir : Thankyou Professor!
12:41:00	 From Dannyel Dani : thanks
12:41:01	 From 주지환 : 감사합니다.
12:41:55	 From 20203491 이재완 : 감사합니다.
12:41:57	 From 20185051 SoJung : 감사합니다~!!
12:42:11	 From 20203491 이재완 : 그럼 다음주에 수업 없는건가요?
12:42:34	 From WonhongYoo : 감사합니다~
12:42:41	 From 20203491 이재완 : 감사합니다
