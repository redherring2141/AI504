10:32:34	 From Jihwan Joo : 안녕하세요
10:32:39	 From 20185051 김소정 : 안녕하세요~
10:32:43	 From Mira Heo : 음악이 경쾌하네요
10:32:43	 From Andrés Brito : Good morning
10:34:51	 From 20208220 이원준 : 교수님, Project 1을 보니까 최종 파일을 tar.gz로 압축하라고 되어있는데 tar.gz 파일을 윈도우에서 압축할 수 있나요?
10:58:46	 From Youngjin Jin : Is it a convention to use filters with the same width and height? Or can we also choose to use filters with different widths and heights?
10:59:24	 From Rushda Basir : 1 is obtained by adding all three channels?
10:59:52	 From M-2019-4421 백유미 : can you explain why activation map become 28X28?
10:59:58	 From Rushda Basir : ok. Thankyou
11:00:07	 From 이승준 : How can we choose a good filter number in CNN process?
11:01:02	 From 이승준 : thank you!
11:02:23	 From jaegyunkim : What if each filter has different size?
11:06:42	 From 이성준 : receptive field 의미가 무엇인가요?
11:07:29	 From 이성준 : 네
11:11:19	 From jaegyunkim : Does the same size many filters have different meaningful filtering? 
How does it guarantee that different filters do different jobs?
11:11:54	 From 이성준 : 32 layer 인건가요?
11:12:34	 From 이성준 : 1 layer의 32개 필터인건가요?
11:14:27	 From jaegyunkim : 감사합니다
11:14:38	 From 이성준 : convolution layer의 필터 수가 10개여서각 CONV에 10개의 그림이 보이는 건가요
11:16:05	 From 이성준 : 네
11:18:32	 From 이승준 : During the process of changing from Matrix structure(Conv) to vectorization(FC) for classification, I wonder spatial information doesn't lose?
11:22:30	 From 박영서 : stride 적용으로 기대할 수 있는 효과 같은것이 있나요?
11:23:06	 From 이성준 : 교수님, 이승준님 질문에 답변해 주신 것 한번 더 말씀해 주실 수 있을까요?
11:24:15	 From 이성준 : 네, 감사합니다.
11:25:30	 From 20185051 SoJung : padding에는 보통 0만 사용하나요?
11:26:37	 From 20185051 SoJung : 감사합니다
11:27:35	 From 20205503 Kyeonghyeon Park : filter에도 bias parameter가 있나요?
11:28:18	 From 20205503 Kyeonghyeon Park : 아 놓쳤네요 감사합니다
11:28:33	 From 20204871 YH Tan : does padding affect the features of image (like the edge or something..)? or the effect is negligible?
11:28:42	 From 20185051 SoJung : 1padding을 사용하면 stride 3로도 나눠지는 것 같은데, stride size를 높이기 위해 padding을 사용하기도 하나요?
11:29:35	 From 20185051 SoJung : 감사합니다
11:29:59	 From JunMin Lee : 32*32*10
11:30:05	 From 20209007 최상범 : 32.32.10
11:30:06	 From Yoonjin Chung : 32x32x10
11:30:50	 From Yoonjin Chung : 250
11:30:54	 From Mario Choi : 750
11:30:57	 From 20209007 최상범 : 5.5.3.10
11:30:57	 From JunMin Lee : 10*(3*5*5 +1) = 760
11:33:39	 From 20208220 이원준 : 제가 잠깐 졸아서 못 들었는데 bias는 FC layer에서 적용되는건가요 아니면 conv 할때마다 계속 적용되나요?
11:33:43	 From 20208220 이원준 : 죄송합니다
11:34:28	 From 20208220 이원준 : 그러면 그 bias인 params 1개가 모든 픽셀마다 적용된다는 말씀이신거죠?
11:35:58	 From 20208220 이원준 : 아아 감사합니다
11:36:34	 From 20203210_Jiyoon_Myung : 5*5 image와 5*5filter를 dot product하면 scalar 값이 나오나요?
11:37:07	 From 박영서 : CNN 논문은 주로 square image(N*N image)에 대해서 다루는데 rectangular image에 대해서 다룰 때도 filter size가 square인 것이 좋은가요?
11:39:01	 From 이성준 : feedforwar와 같은 역할이 되나요?
11:39:08	 From 이성준 : 1x1필터
11:40:37	 From 이성준 : 아니 질문 잘못되었네요
11:41:00	 From 이성준 : 질문 취소합니다.
11:41:20	 From 박영서 : 감사합니다!
11:47:08	 From 이성준 : 아래 의미가 맞나요?Smaller filter --> small F (필터크기 F x F x depth)deeper filter --> large K (필터 수 K)
11:47:09	 From Rushda Basir : Doesnot loss of spatial information in pooling effect results?
11:47:47	 From 이재훈 : 어떤식으로든 맨 마지막에 classification task 를 수행하려면 Fully connected layer 를 거쳐야하는건가요~?
11:49:25	 From 이성준 : deeper architecture가 필터수가 많은 것인지 layer가 많은 것인지 질문드립니다.
11:50:01	 From 이성준 : 네
11:50:23	 From 이재훈 : 감사합니다
11:53:14	 From 이성준 : 여기서는 채널이 무엇을 의미하는지 궁금합니다.
11:53:52	 From 이성준 : 최초입력에서는 채널이 1개인가요?
11:54:08	 From 이성준 : 그림에서 여러개여서요
11:54:11	 From 이성준 : 네
11:54:30	 From jaegyunkim : 아마 여러가지 다른방법으로 임베딩된 벡터일꺼같습니다.
11:54:42	 From 이성준 : 네, 감사합니다.
12:04:15	 From 20185051 SoJung : 처음부터 적은 개수의 filter를 사용하는 것과 무슨 차이가 있는지 잘 모르겠습니다.
12:05:13	 From 20204507 이현기 : 학습을 할때만 dropout하는지 궁금합니다
12:05:20	 From 20208220 이원준 : 그럴거에요
12:05:22	 From jaegyunkim : 네
12:05:34	 From 20204507 이현기 : 감사합니다
12:05:34	 From 20185051 SoJung : 감사합니다
12:06:06	 From 20209007 최상범 : no just for preventing overfitting I thought dropout was also a one of technique for pruning
12:07:31	 From 이찬규 : Is the probability to dropout a fixed value?
12:08:00	 From 20208220 이원준 : 제가 검색해봤을땐 그랬던것 같습니다
12:08:59	 From M_20203621_주신영 : Project 1 관련하여 질문있습니다. 1. 코드나 모델 아키텍쳐는 제출 안해도 되고, 생성한 이미지만 압축해서 제출하면 되는건가요? 2. DCGAN 을 사용해도 되나요?
12:09:44	 From 51560 김성민 : training시 사용한 샘플에 대한 normalization할때 의 E(x)와 V(x)로 test할때 그대로 사용하는건지 궁금합니다.
12:12:12	 From soro bedionita : should we used convnet
12:12:16	 From M_20203621_주신영 : 감사합니다
12:12:32	 From Joshua Julio _20204805 : when is the project 1 due?
12:12:36	 From Yoonjin Chung : 클라썸에서 Project 1에 대한 내용이 안 보이는데 어디서 볼 수 있을까요?
12:12:44	 From 51560 김성민 : 네 답변 감사합니다!
12:12:47	 From 김인희 : Classum에 project 1 관련 공지가 올라가 있나요?
12:13:04	 From 20203210_Jiyoon_Myung : klms에있습니다
12:13:10	 From 20185051 SoJung : KLMS로 제출하는건가요?
12:14:32	 From 이성준 : project 어떻게 진행해야 하는지 감이 잡히지 않는데, 이번주 practice 들으면 더 접근할 수 있을까요?
12:15:12	 From Sungwon Kim : 네이버 임직원은 어떻게 제출해야하나요??
12:15:53	 From 20185051 SoJung : 생성한 image의 format은 상관없나요?
12:16:16	 From 20185051 SoJung : 감사합니다
12:16:23	 From yongdae kim : FID 20.0 이하로 안되면 어떻게 하나요?? 
12:16:44	 From yongdae kim : 감사합니다 
12:17:08	 From Jisu : dont we need to know upsampling for decoder?
12:17:15	 From Jisu : like transposed convolution
12:17:17	 From _ : 1000개이상이 넘어가야 보통 fid가 20이하로 되나요?
12:17:37	 From _ : 감사합니다!
12:17:55	 From _ : dcgan uses convtranspose2d
12:18:06	 From 51560 김성민 : 프로젝트 채점기준이 궁금합니다. 모델의 결과 FID score만 반영되나요?
12:18:43	 From 51560 김성민 : 네 답변 감사합니다
12:19:11	 From _ : colab 에서 cuda memory error 등이 일어날때 해결방법 추천해주실만한거 있나요?
12:19:16	 From soro bedionita : can we evaluate the FID score before submission?
12:20:59	 From 20208220 이원준 : Let's go through before 1pm!
12:22:23	 From 20208220 이원준 : Thank you!
12:23:24	 From 20185051 SoJung : 감사합니다!
12:23:29	 From Jihwan Joo : 감사합니다.
12:23:51	 From 20204871 YH Tan : Thank you!
12:23:52	 From 20205503 Kyeonghyeon Park : 다음 시간에 internal distribution shift부터 다시 설명 부탁드려도 될까요? danger 한 이유를 다시 듣고 싶습니다
12:23:52	 From Andrés Brito : Thank you
12:23:59	 From Rushda Basir : Thankyou
12:24:00	 From 51560 김성민 : 감사합니다!
12:24:01	 From 20203644 최상현 : 감사합니당
12:24:04	 From 20205503 Kyeonghyeon Park : 감사합니다
12:24:04	 From mjh to Yoonjae Choi (Privately) : 감사합니다!
12:24:06	 From 20203181 김현성 : 감사합니다
12:24:06	 From Daniel Saatchi : Thanks
12:24:07	 From 20204225 강정모 : 감사합니다!
12:24:15	 From 20194321 김범진 : 감사합니다
