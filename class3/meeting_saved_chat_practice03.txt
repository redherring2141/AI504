10:29:13	 발신자 [TA] Jihoon Tack : 제가 윈도우에서 실행을 해본 적이 없어서, 혹시 사용해보신 분이 있다면 설명해주시면 좋을 것 같습니다ㅜ
10:29:45	 발신자 20198221 한건호 : jupiter notebook에서도 GPU 사용이 가능한가요?
10:29:46	 발신자 20185051 SoJung : 다운 받은 수업자료를 어떻게 실행시켜야 하는지 모르겠습니다.
10:29:54	 발신자 이재훈 : 보통 윈도우에서는 주피터 노트북의 default working directory 가 'C:\Users\유저이름' 으로 알고 있습니다. 여기에 다운받으신 ipynb 파일을 카피하셔도 되고, 
10:29:54	 발신자 20205440 (Will)mer Quiñones : on windows, open Anaconda prompt in your location and type 'jupyter notebook'
10:30:41	 발신자 이재훈 : 아니면 노트북 실행시키신 다음에 우측 상단에 upload 라는 버튼이 있는데 그거 클릭하셔서 다운받으신 파일 경로에 가서 업로드 하시면 주피터 file 목록에 뜹니다
10:31:01	 발신자 20185051 SoJung : 아!! 이재훈님, 감사합니다!
10:36:12	 발신자 jaegyunkim : 싱크에 문제가 있어서 어디서 GPU를 키는지 다시한번 보여주시겠어요?
10:36:26	 발신자 20198221 한건호 : 주피터 환경에서도 GPU 사용을 위해 설정해야 하나요?
10:36:32	 발신자 20204618 황시환 : 런타임 -> 런타임 유형변경 -> None에서 GPU로 변경
10:36:45	 발신자 jaegyunkim : 감사합니다
10:38:16	 발신자 20205440 (Will)mer Quiñones : (ENG) To select GPU on colab:Runtime -> Change runtime type -> Select GPU
10:48:02	 발신자 Jisu : You can use .cuda() instead of .to(‘cuda’). Same goes to .cpu(). It makes syntax much simpler
10:49:11	 발신자 Rushda Basir : what does requires_grad does?
10:51:34	 발신자 20203210_Jiyoon_Myung : grad()를 하면 직전에 backward()를 실행한 텐서에 대한 gradient가 나오는건가요?
10:52:17	 발신자 20208220 이원준 : What is difference between .retain_grad and .backward
10:52:22	 발신자 이재완 : y.retain_grad에서 retain_grad는 수식만 보여주는건가요?
10:53:20	 발신자 하영 장 : 'with torch.no_grad():' 이거 쓰지 않고x에 requires_grad=False로 하면 어떻게 되나요?
10:53:32	 발신자 유명성 : y, z는 왜 retain_grad()이고, out은 왜 backward()인가요?
10:54:19	 발신자 51560 김성민 (청강생) : back propagation 에서 실행순서가y.retain_grad()z.retain_grad()out.backward()처럼 앞에서부터 y->z->out 순으로 하는건가요?
10:54:46	 발신자 20208220 이원준 : okay
11:00:46	 발신자 20205440 (Will)mer Quiñones : later you could watch this video about how PyTorch stores the grad:https://www.youtube.com/watch?v=MswxJw-8PvE
11:01:02	 발신자 Hyun-Kyung Kim : Thanks
11:02:24	 발신자 20203210_Jiyoon_Myung : W와 b는 랜덤으로 설정되는건가요?
11:07:39	 발신자 Hyun-Kyung Kim : nn.Linear는 matrix를 생성하는 것인가요?x=self.lineaer_1(x)에서 (x)는 왜 필요한가요'?
11:08:48	 발신자 Yoonjae Choi : nn.Linear는 weight matrix와 bias term을 같이 생성합니다.
11:09:39	 발신자 Yoonjae Choi : self.linear_1자체가 함수로 볼 수 있기 때문에 입력을 줘야 출력이 나옵니다. 그래서 (x)를 붙여줍니다.
11:09:52	 발신자 51560 김성민 (청강생) :         x = self.linear_1(x)        x = self.relu(x) # Activation function        x = self.linear_2(x)에서 linear_뒤에 붙는 숫자는 순서대로 1, 2, 3 으로 만들어지는 것인가요?
11:10:11	 발신자 Joshua Julio _20204805 : why do we only shuffle the train set and not the test set?
11:10:45	 발신자 jaegyunkim : dataloader의 셔플은 배치 안에서만 이루어지는 것이죠? 전체 셋이 아닌
11:10:58	 발신자 Yoonjae Choi : x = self.linear_1(x)를 하는 것은 하나의 벡터 하나를 linear layer를 통과시키는 것과 동일합니다.
11:11:06	 발신자 20185051 SoJung : 실행했을 때 나오는 %는 정확도인건가요?
11:11:58	 발신자 jaegyunkim : 감사합니다
11:11:58	 발신자 20185051 SoJung : 아, 데이터 로딩 정도인가보네요..ㅎ;;
11:12:05	 발신자 Yoonjae Choi : Accuracy라고 써있다면 정확도 맞습니다.
11:12:18	 발신자 20185051 SoJung : 네! 감사합니다
11:15:56	 발신자 Rushda Basir : How do we choose value of lr?
11:16:54	 발신자 Rushda Basir : oh ok. Thankyou.
11:19:06	 발신자 Rushda Basir : what does .zero_grad() does?
11:19:19	 발신자 jaegyunkim : 모델 마지막에 softmax과 같은 activation없이 정의하는게 맞나요?
11:19:26	 발신자 김낙일 : optimizer.zero_grad() 를 epoch 마다하게 되면 배치 단위가 아니라 전체 epoch에 대해 gradient를 계산하게 되는 건가요?
11:19:31	 발신자 유명성 : 배치 사이즈는 어떻게 정하나요? 너무 큰 값이거나, 작은 값이면 문제가 있나요?
11:20:28	 발신자 20185051 SoJung : 넵
11:20:28	 발신자 jonghyeon : 네
11:20:29	 발신자 김무종 : 네
11:20:29	 발신자 이재완 : 네
11:20:30	 발신자 KAIST SoyeonJung : yes
11:20:33	 발신자 20205440 (Will)mer Quiñones : if you don't use 'zero_grad()', it will accumulate all the gradients from previous steps (epochs)
11:20:45	 발신자 Joshua Julio _20204805 : how do we optimize the number of epochs?
11:29:14	 발신자 M-2019-4421 백유미 : WX^를 계산하여 나온 y^의 하나의 dimension값은 WX^를 내적한 값인가요?
11:30:50	 발신자 이성준 : 백유미님 질문에서의 예제에서, 라벨없는데 어떻게 W 구한 건가요?
11:30:54	 발신자 20185051 SoJung : epoch가 글씨 0~9까지를 각각 의미하게 되는건가요?
11:31:39	 발신자 Ju Yunsang : softmax에서 가장 큰 prob이 나온 값은 linear에서도 가장 크게 나올 것 같은데요
11:31:43	 발신자 이성준 : linear.nn의 예
11:32:01	 발신자 Ju Yunsang : softmax를 쓰지 않고 linear로 마무리하고 NLLloss로 학습을 해도 무방한건가요??
11:32:31	 발신자 하영 장 : 코랩으로 train을 하게 되면 torch.size([128,784)가  step마다 나오는데..저만 나오나요?
11:33:05	 발신자 20204225 강정모 : print (images.shape)를 주석처리 하시면 돼요
11:33:09	 발신자 M-2019-4421 백유미 : 감사합니다.
11:33:16	 발신자 하영 장 : 감사합니다
11:33:17	 발신자 이성준 : 처음 예제에서
11:33:57	 발신자 구인용 20203022 : @낙일님 batch의 gradient가 accumulate 되어 전체 epoch의 gradient를 구할 수 있겠지만, optimizer.step이 batch마다 일어나기 때문에 업데이트가 비정상적으로 진행될 것 같습니다.
11:33:58	 발신자 이성준 : 저도 스텝마다
11:34:31	 발신자 [TA] Jihoon Tack : NLLloss를 사용하시려면 softmax를 취하고 사용하시면 됩니다
11:35:41	 발신자 Ju Yunsang : 감사합니다
11:39:10	 발신자 M-2019-4421 백유미 : 최적의 layer수는 직접 해보면서 결정해야 하나요?
11:39:12	 발신자 20193321 신은택 : optimizer.zero_grad(), loss.backward() , optimizer.step()  에 대해 설명해 주실 수 있을까요?
11:39:15	 발신자 20204225 강정모 : self.fc가 의미하는게 무엇인가요?
11:39:28	 발신자 20204225 강정모 : 감사합니다
11:39:42	 발신자 최환일(20204597) : torch.no_grad()하는것과 model.eval() 하는게 같은 역할인가요? 
11:39:52	 발신자 20185051 SoJung : 이미지가 총 469개가 있는거 맞죠?
11:40:31	 발신자 유명성 : cross-entropyLoss 적용할 때 마지막 레이어에서 softmax를 안쓰는건, pytorch에서만 그런거죠?
11:40:38	 발신자 20208220 이원준 : 그 outputs는 그러니까 크기가 128또는 100(배치크기) * 10(0~9) 인거죠?
11:41:39	 발신자 20208220 이원준 : predicted는 그러면 크기가 어떻게 되나요?? 128*1?
11:42:25	 발신자 20208220 이원준 : _,predicted 라고 한 이유가 있나요?
11:43:37	 발신자 20208220 이원준 : 저걸 _,predicted라고 안하고 그냥 predicted라고 하면
11:43:41	 발신자 Joshua Julio _20204805 : what is momentum hyperparameter?
11:43:44	 발신자 Andrés Brito : Can we see the weight values after the optimization?
11:43:47	 발신자 20208220 이원준 : 100*1이 나오는건데 _라고 해서 그냥 벡터가 나오는거죠?
11:43:52	 발신자 20193321 신은택 : loss.backward()를 해서 optimizer.step()하는 부분이 잘 이해가 안되서 설명해 주실 수 있을까요?
11:43:56	 발신자 Rushda Basir : how to you check minibatch size?
11:45:33	 발신자 Yonghee Kim : CPU memory to GPU memory는 느리기 때문에, to('cuda') 호출 횟수를 최소화하는게 좋다고 알고있는데요. 매 iteration마다 to('cuda')를 호출해도 상관이 없나요? 혹시 내부적으로 GPU memory cache가 될까요?
11:45:48	 발신자 Andrés Brito : ok thank you
11:46:13	 발신자 20209007 최상범 : same question as Yonghee Kim
11:46:42	 발신자 박성진 (20204341) : Instead of using 'outputs.data', it doesnt matter using just 'outputs' ? 
11:46:44	 발신자 20208220 이원준 : 제꺼 아직 설명 안해주셨는데 마저 설명해주실 수 있나요?
11:48:36	 발신자 20208220 이원준 : _,predicted 관련해서 질문했습니다
11:49:19	 발신자 이재완 : model.eval()이랑 with torch.no_grad() 차이점 다시한번 설명해주실 수 있으신가요?
11:49:37	 발신자 20208220 이원준 : 아 감사합니다
11:50:22	 발신자 이성준 : (linear_fn  # WX + b)
11:50:25	 발신자 이성준 : # input dim 3, output dim 1linear_fn = nn.Linear(3, 1)
11:50:43	 발신자 이성준 : 이 부분에서 라벨이 없는데 W를 어떻게 알아내는지 질문드립니다.
11:51:04	 발신자 20205440 (Will)mer Quiñones : torch.no_grad -> set 'requires_grad' to falsemodel.eval() -> affects batchnorm and dropout layers
11:53:05	 발신자 20185051 SoJung : 그럼 학습된 결과는 아니고
11:53:16	 발신자 20185051 SoJung : 그냥 임의로 계산을 했다는걸 말씀해주시는거죠..?
11:54:25	 발신자 M-2019-4421 백유미 : 함수가 linear하지 않은 경우는 어떻게 적용할 수 있나요? 
11:55:13	 발신자 20203100 Sunkyoung Kim : train loader에서 batch size를 넣을 때, 만약 gpu를 여러개 사용하게 되면 train loader에 넣는 batch_size는 각 gpu당 사용되는 batch인건가요? 아니면 전체 학습에 사용되는 total batch size인건가요??
11:55:17	 발신자 M-2019-4421 백유미 : 커스터마이즈는
11:55:25	 발신자 M-2019-4421 백유미 : 액티베이션 함수를 적용하는건가요?
11:56:07	 발신자 이성준 : 숫자 맞추기 예제에서 (0~9) 숫자별로 학습으로부터 W벡터를 구하는 것인지, 즉 W가 (128 by 10) 매트릭스인건가요?
12:01:36	 발신자 M-2019-4421 백유미 : W1, W2가 연결되어 있는데, W1, W2 업데이트할때 각각을 독립적으로 계산해도 되나요?
12:02:08	 발신자 M-2019-4421 백유미 : 이미 포함되어 있네요.
12:02:12	 발신자 M-2019-4421 백유미 : 감사합니다
12:02:52	 발신자 M-2019-4421 백유미 : 그럼 위에서 optimizer.zero_grad()는 어떤 이유로 진행하나요?
12:03:08	 발신자 20203100 Sunkyoung Kim : 답변 감사합니다!
12:04:18	 발신자 20185051 SoJung : 저도 optimizer.zero_grad() 이부분 설명을 이해 못했습니다.
12:04:24	 발신자 이성준 : 네
12:06:00	 발신자 20185051 SoJung : 감사합니다!
12:06:06	 발신자 M-2019-4421 백유미 : 감사합니다
12:06:43	 발신자 M-2019-4421 백유미 : customization에 대해서
12:06:48	 발신자 M-2019-4421 백유미 : 설명해주실 수 있으신지요?
12:07:00	 발신자 M-2019-4421 백유미 : nonliinear
12:12:56	 발신자 20185051 SoJung : 보통 러닝을 돌리면 실행 할때마다 확률값이 다른데, 몇 번이나 반복해서 평균해야 이 정도 확률이다 라고 말할 수 있는건가요?
12:13:14	 발신자 이재완 : model.eval()은 batch normalization 이나 normalziation 할때 사용한다고 하셨는데 test시 model.eval()을 사용하면 문제가 있을까요?
12:14:49	 발신자 이재완 : 감사합니다
12:15:14	 발신자 20204225 강정모 : 감사합니다
12:15:15	 발신자 20185051 SoJung : 감사합니다!
12:15:15	 발신자 M-2019-4421 백유미 : 감사합니다~!!
12:15:19	 발신자 이성준 : 감사합니다.
12:15:21	 발신자 Minkyu Jeong : 감사합니다!
12:15:26	 발신자 Sungwon Kim : 감사합니다~
