{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI504_Word2Vec_qus",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnTxUxjxmVM8"
      },
      "source": [
        "# **Week 9: Word2Vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r1U6-otmXcE"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1Ren0qgAnsDGkgN_cnfCcE66GZAZfhMAp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzqPIQn_meW2"
      },
      "source": [
        "## **1.  We will implement Word2Vec. (Skip-gram)**\n",
        "1-1. Define Tokenizer : For training Word2Vec, Must tokenize sentences. <br> \n",
        "1-2. Build Vocabulary : For preventing overfitting, Set min frequency of word <br>\n",
        "1-3. Define Skipgram <br>\n",
        "1-4. Implement Word2Vec with proper loss function.\n",
        "\n",
        "## **2.  We will implement SentencePiece and Word2Vec.**\n",
        "2-1. SentencePiece <br>\n",
        "2-2. Word2Vec\n",
        "\n",
        "## **3.  HOMEWORK**\n",
        "3-1. MosesTokenizer && BPE (Mainly used in translation) <br><br>\n",
        "\n",
        "If you have any questions, feel free to ask\n",
        "\n",
        "*   E-Mail Address : seongjunyang@kaist.ac.kr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPoRvzpGmhxD"
      },
      "source": [
        "## **1. Implement Word2Vec**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSQM98qSmlf_"
      },
      "source": [
        "### **1) Import required packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ6ShDmdmctU"
      },
      "source": [
        "from abc import ABC\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Dict, Tuple\n",
        "from random import randint\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSB8tVngmn3G"
      },
      "source": [
        "### **2) Make Tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z8-oq88mp1m"
      },
      "source": [
        "#Implement simple sentence tokenizer\n",
        "#Punctuation marks should be a seperate token: . , ! ?\n",
        "#For this tokenizing function, 're' library might be helpful but it is not mandatory.\n",
        "\n",
        "def tokenize(sentence):\n",
        "\n",
        "    ###################################################\n",
        "    # TODO: Separate tokens by . , ! ?\n",
        " \n",
        "    ###################################################\n",
        "    assert type(tokens) == list # tokens must be a list of words. \n",
        "    return tokens\n",
        "\n",
        "#Example: 'Don\\'t be fooled, but be clever.'\n",
        "#==> ['Don\\'t', 'be', 'fooled', ',', 'but', 'be', 'clever', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PXGpxHAmv8I"
      },
      "source": [
        "### **3) Tokenizing Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr0dEn58mzeO"
      },
      "source": [
        "def test_tokenize(sentence, hypothesis):\n",
        "  tokens = tokenize(sentence)\n",
        "  print(\"## Your tokenized result : \", tokens)\n",
        "  assert tokens == hypothesis, \\\n",
        "    \"Your tokenized list do not match expected result\"\n",
        "  print(\"Tokenizing test passed!\")\n",
        "\n",
        "sentence1 = \"This sentence should be tokenized properly.\"\n",
        "answer1 = ['This', 'sentence', 'should', 'be', 'tokenized', 'properly', '.']\n",
        "\n",
        "sentence2 = \"Jhon's book is not popular, but he loves his book.\"\n",
        "answer2 = [\"Jhon's\", \"book\", \"is\", \"not\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]\n",
        "\n",
        "sentence3 = \"  .,! ?,,'-4.  ! \"\n",
        "answer3 = ['.', ',', '!', '?', ',', ',', \"'-4\", '.', '!']\n",
        "\n",
        "test_tokenize(sentence1, answer1)\n",
        "test_tokenize(sentence2, answer2)\n",
        "test_tokenize(sentence3, answer3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1iIxwJCm2XI"
      },
      "source": [
        "### **5) Bulid Vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHxof7Kfm4o8"
      },
      "source": [
        "#Vocabulary Builder\n",
        "\n",
        "#Implement vocab builder that makes word2idx and idx2word from sentences.\n",
        "#Words with too few frequencies can cause over-fitting, so we will replace these words to <UNK> tokens.\n",
        "#Because negative sampling needs the frequency of each word, you will calculate this also in here.\n",
        "#<PAD> token will be used later for batching, but don't mind it now.\n",
        "\n",
        "#Hint: Counter in collection library would be helpful\n",
        "\n",
        "'''\n",
        "    Arguments:\n",
        "    sentences -- The list of sentence to build vocab\n",
        "    min_freq -- The minimum frequency of a word to be a separate token.\n",
        "                A word whose frequency is less than this number should be treated as <UNK> token.\n",
        "\n",
        "    Return:\n",
        "    idx2word -- A list which takes a index and gives its matched word   ex) ['<PAD>', '<UNK>', 'This', 'sentence', 'be', 'tokenized', 'propery', '.', 'Jhon', \"'s\", 'book', 'is', 'not', 'popular', ',', 'but', 'he', 'loves', 'his']\n",
        "    word2idx -- A dictionary which maps a word to its indices   ex) {'<PAD>': 0, '<UNK>': 1, 'This': 2, 'sentence': 3, 'be': 4, 'tokenized': 5, 'propery': 6, '.': 7, 'Jhon': 8, \"'s\": 9, 'book': 10, 'is': 11, 'not': 12, 'popular': 13, ',': 14, 'but': 15, 'he': 16, 'loves': 17, 'his': 18}\n",
        "    word_freq -- The list of the number of appearance count of each word through entire sentences.   ex) [0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "                 The frequency of <UNK> token should be calculated properly.\n",
        "'''\n",
        "\n",
        "def build_vocab( sentences: List[List[str]], min_freq: int) -> Tuple[List[str], Dict[str, int], List[int]]:\n",
        "    \n",
        "    # Special tokens, insert these tokens properly\n",
        "    PAD = '<PAD>'\n",
        "    PAD_idx = 0 \n",
        "    UNK = '<UNK>'\n",
        "    UNK_idx = 1 \n",
        "\n",
        "\n",
        "    word2idx = {PAD: PAD_idx, UNK: UNK_idx}\n",
        "    idx2word = {PAD_idx: PAD, UNK_idx: UNK} \n",
        "    \n",
        "    ############################################################################\n",
        "    # TODO : Fill ???? with proper code\n",
        "\n",
        "    # define lambda funtion\n",
        "    flatten = ???? # hint : [[\"This\", \"sentence\", \"be\", \"tokenized\", \"propery\", \".\"], [\"Jhon\", \"'s\", \"book\", \"is\", \"not\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]] \n",
        "                   #        -> ['This', 'sentence', 'be', 'tokenized', 'propery', '.', 'Jhon', \"'s\", 'book', 'is', 'not', 'popular', ',', 'but', 'he', 'loves', 'his', 'book', '.']\n",
        "    word_freq = dict(Counter(flatten(sentences)))\n",
        "\n",
        "    # Plus word not in word2idx\n",
        "    for word_item in word_freq.items():\n",
        "        ????\n",
        "    \n",
        "    # Delete word below min_freq\n",
        "    for word_item in word_freq.items():\n",
        "        ????\n",
        "\n",
        "    # After deleting word, reindex word2idx\n",
        "    for idx, word_item in enumerate(word2idx.items()):\n",
        "        ????\n",
        "\n",
        "\n",
        "    word_freq = {PAD: 0, UNK: 0}\n",
        "    # Find word frequency again and if word not in word2idx, it is UNK (Unknown token).\n",
        "    for word_list in sentences: # sentences are list of tokenized sentence.\n",
        "        ????\n",
        "\n",
        "    word_freq = ????\n",
        "    idx2word = ????\n",
        "    ############################################################################\n",
        "\n",
        "    assert idx2word[PAD_idx] == PAD and word2idx[PAD] == PAD_idx, \\\n",
        "        \"PAD token should be placed properly\"\n",
        "    assert idx2word[UNK_idx] == UNK and word2idx[UNK] == UNK_idx, \\\n",
        "        \"UNK token should be placed properly\"\n",
        "    assert len(idx2word) == len(word2idx) and len(idx2word) == len(word_freq), \\\n",
        "        \"Size of idx2word, word2idx and word_freq should be same\"\n",
        "\n",
        "    return idx2word, word2idx, word_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbCSBIWVm-Gf"
      },
      "source": [
        "### **6) Test build_vocab function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoC0eR7znSq0"
      },
      "source": [
        "def test_vocab_builder():\n",
        "    print(\"======Vocabulary Builder Test Cases======\")\n",
        "    \n",
        "    ##################################### First test #####################################\n",
        "    sentences = [[\"This\", \"sentence\", \"be\", \"tokenized\", \"propery\", \".\"],\n",
        "                 [\"Jhon\", \"'s\", \"book\", \"is\", \"not\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]]\n",
        "\n",
        "    idx2word, word2idx, word_freq = build_vocab(sentences, min_freq=1)\n",
        "    assert sentences == [[idx2word[word2idx[word]] for word in sentence] for sentence in sentences], \\\n",
        "        \"Your word2idx and idx2word do not show consistency\"\n",
        "    print(\"idx2word : \", idx2word)\n",
        "    print(\"word2idx : \", word2idx)\n",
        "    print(\"word_freq\", word_freq)\n",
        "    print(\"The first test passed!\")\n",
        "    print()\n",
        "     \n",
        "    \n",
        "    ##################################### Second test #####################################\n",
        "    sentences = [['a', 'a', 'b'], ['b', 'c']]\n",
        "\n",
        "    idx2word, word2idx, word_freq = build_vocab(sentences, min_freq=1)\n",
        "\n",
        "    assert len(idx2word) == 5 and \\\n",
        "           word_freq[word2idx['<UNK>']] == 0 and word_freq[\n",
        "               word2idx['<PAD>']] == 0 and \\\n",
        "           word_freq[word2idx['a']] == 2 and word_freq[word2idx['b']] == 2 and word_freq[word2idx['c']] == 1, \\\n",
        "        \"Result of word_freq do not match expected result\"\n",
        "    print(\"idx2word : \", idx2word)\n",
        "    print(\"word2idx : \", word2idx)\n",
        "    print(\"word_freq\", word_freq)\n",
        "    print(\"The second test passed!\")\n",
        "    print()\n",
        "    \n",
        "    ###################################### Third test ######################################\n",
        "    sentences = [[\"a\", \"b\", \"c\", \"d\", \"e\"],\n",
        "                 [\"c\", \"d\", \"f\", \"g\"],\n",
        "                 [\"d\", \"e\", \"g\", \"h\"]]\n",
        "\n",
        "    idx2word, word2idx, word_freq = build_vocab(sentences, min_freq=2)\n",
        "    assert set(word2idx.keys()) == {'<PAD>', '<UNK>', 'c', 'd', 'e', 'g'} and len(idx2word) == 6 and \\\n",
        "           word_freq[word2idx['c']] == 2 and word_freq[word2idx['d']] == 3 and word_freq[word2idx['e']] == 2 and \\\n",
        "           word_freq[word2idx['g']] == 2 and \\\n",
        "           word_freq[word2idx['<UNK>']] == 4 and word_freq[word2idx['<PAD>']] == 0, \\\n",
        "        \"Your vocabulary builder does not work with min_freq argument\"\n",
        "    \n",
        "    print(\"idx2word : \", idx2word)\n",
        "    print(\"word2idx : \", word2idx)\n",
        "    print(\"word_freq\", word_freq)\n",
        "    print(\"The third test passed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ754VoFnVto"
      },
      "source": [
        "test_vocab_builder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp7JsP-_nYk2"
      },
      "source": [
        "### **7) Define Skipgram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq43sDZynYn8"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1z3fcVn_S4HKj8TXf4Recs8ATOxK8yCh5)\n",
        "*   3 is just an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD0jLur5ndAB"
      },
      "source": [
        "\"\"\" \n",
        "Function to generate a (center_word, outside_words) pair for skipgram\n",
        "\n",
        "Implement the function that generate a (center_word, outside_words) pair from the given sentence and the location of center word.\n",
        "\n",
        "Argument:\n",
        "  sentence -- A sentence where the center word and the outside words come from (type : List[str])\n",
        "  window_size -- Integer, context window size (type : int)\n",
        "  center_word_loc -- The location of the center word within the sentence (type : int)\n",
        "\n",
        "Return:\n",
        "  center_word -- String, a center word\n",
        "  outside_words -- List of string, words within the window centered on the center word.\n",
        "                    outside_words dose not include center_word.\n",
        "                    The number of outside words could be less than 2 * window_size.\n",
        "\"\"\"\n",
        "\n",
        "def skipgram( sentence, window_size, center_word_loc):\n",
        "    \n",
        "    ############################################################################\n",
        "    # TODO : Define skipgram\n",
        "    outside_words = []\n",
        "\n",
        "    # Add proper context_word by looping by window size based on the center word\n",
        "\n",
        "    ############################################################################\n",
        "    assert type(sentence[center_word_loc]) == str and type(outside_words) == list\n",
        "    return sentence[center_word_loc], outside_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElLy4Rl5nkiv"
      },
      "source": [
        "### **8) Test Skipgram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrnDFrcInmwp"
      },
      "source": [
        "def test_skipgram():\n",
        "    print(\"======Skipgram Test Cases======\")\n",
        "\n",
        "    ##################################### First test #####################################\n",
        "    sentence = [\"Jhon's\", \"book\", \"is\", \"not\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]\n",
        "    center_word, outside_words = skipgram(sentence, window_size=2, center_word_loc=3)\n",
        "    assert center_word == 'not' and len(outside_words) == 4 and set(outside_words) == {\"book\", \"is\", \"popular\", \",\"}, \\\n",
        "        \"Your skipgram does not work\"\n",
        "    \n",
        "    print(\"center word : \", center_word)\n",
        "    print(\"outsied_words : \", outside_words)\n",
        "    print(\"The first test passed!\")\n",
        "    print()\n",
        "\n",
        "    ##################################### Second test #####################################\n",
        "    sentence = [\"Jhon's\", \"book\", \"is\", \"not\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]\n",
        "    center_word, outside_words = skipgram(sentence, window_size=3, center_word_loc=1)\n",
        "    assert center_word == 'book' and len(outside_words) == 4 and set(outside_words) == {\"Jhon's\", \"is\", \"not\", \"popular\"}, \\\n",
        "        \"Your skipgram does not work when the front of window is cut out\"\n",
        "\n",
        "    print(\"center word : \", center_word)\n",
        "    print(\"outsied_words : \", outside_words)\n",
        "    print(\"The second test passed!\")\n",
        "    print()\n",
        "\n",
        "    ##################################### Third test #####################################\n",
        "    sentence = [\"Jhon's\", \"book\", \"is\", \"not\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]\n",
        "    center_word, outside_words = skipgram(sentence, window_size=2, center_word_loc=11)\n",
        "    assert center_word == '.' and len(outside_words) == 2 and set(outside_words) == {\"his\", \"book\"}, \\\n",
        "        \"Your skipgram does not work when the rear of window is cut out\"\n",
        "\n",
        "    print(\"center word : \", center_word)\n",
        "    print(\"outsied_words : \", outside_words)\n",
        "    print(\"The third test passed!\")\n",
        "    print()\n",
        "\n",
        "    ##################################### Forth test #####################################\n",
        "    sentence = [\"Jhon's\", \"book\", \"is\", \"popular\", \".\"]\n",
        "    center_word, outside_words = skipgram(sentence, window_size=3, center_word_loc=2)\n",
        "    assert center_word == 'is' and len(outside_words) == 4 and set(outside_words) == {\"Jhon's\", \"book\", \"popular\", \".\"}, \\\n",
        "        \"Your skipgram does not work when the both side of window is cut out\"\n",
        "\n",
        "    print(\"center word : \", center_word)\n",
        "    print(\"outsied_words : \", outside_words)\n",
        "    print(\"The forth test passed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPb3OWRRnqhx"
      },
      "source": [
        "test_skipgram()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kis7786InsQm"
      },
      "source": [
        "### **8) Implement Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zHcBQoEnusx"
      },
      "source": [
        "class Word2Vec(torch.nn.Module, ABC):\n",
        "    \n",
        "    def __init__(self, n_tokens, word_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        self.center_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
        "        self.outside_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        torch.nn.init.normal_(self.center_vectors.data)\n",
        "        torch.nn.init.normal_(self.outside_vectors.data)\n",
        "    \n",
        "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
        "      return naive_softmax_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices) # It is defined below section.\n",
        "\n",
        "\n",
        "class SkipgramDataset(IterableDataset):\n",
        "    PAD_TOKEN = '<PAD>'\n",
        "    PAD_TOKEN_IDX = 0\n",
        "    UNK_TOKEN = '<UNK>'\n",
        "    UNK_TOKEN_IDX = 1\n",
        "\n",
        "    def __init__(self, path, window_size, min_freq, device=torch.device('cpu')):\n",
        "        self._window_size = window_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            sentences = [tokenize(line.strip().lower()) for line in f]\n",
        "\n",
        "        idx2word, word2idx, word_freq = build_vocab(sentences, min_freq=min_freq)\n",
        "\n",
        "        self._sentences = sentences\n",
        "        self._idx2word = idx2word\n",
        "        self._word2idx = word2idx\n",
        "        self._neg_sample_prob = torch.Tensor(word_freq).to(device) ** .75\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self):\n",
        "        return len(self._idx2word)\n",
        "\n",
        "    def negative_sampler(self, outside_word_indices, K):\n",
        "        indices = outside_word_indices.flatten()\n",
        "\n",
        "        if outside_word_indices.device == torch.device('cpu'):\n",
        "            negatives = []\n",
        "            for index in indices:\n",
        "                temp = self._neg_sample_prob[index].clone()\n",
        "                self._neg_sample_prob[index] = 0.\n",
        "                negatives.append(torch.multinomial(self._neg_sample_prob, num_samples=K, replacement=True))\n",
        "                self._neg_sample_prob[index] = temp\n",
        "            negatives = torch.stack(negatives)\n",
        "\n",
        "        else:\n",
        "            probs = self._neg_sample_prob.repeat(indices.shape[0], 1)\n",
        "            probs.scatter_(dim=-1, index=indices.unsqueeze(-1), src=outside_word_indices.new_zeros([]))\n",
        "            negatives = torch.distributions.categorical.Categorical(probs).sample([K]).T\n",
        "\n",
        "        return negatives.reshape(list(outside_word_indices.shape) + [K]).detach()\n",
        "\n",
        "    def idx2word(self, index: int) -> str:\n",
        "        return self._idx2word[index]\n",
        "\n",
        "    def word2idx(self, word: str) -> int:\n",
        "        return self._word2idx[word]\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            index = randint(0, len(self._sentences) - 1)\n",
        "            sentence = self._sentences[index]\n",
        "            center_word, outside_words = skipgram(sentence, self._window_size, randint(0, len(sentence) - 1))\n",
        "\n",
        "            center_word_index = self._word2idx.get(center_word, SkipgramDataset.UNK_TOKEN_IDX)\n",
        "            outside_word_indices = list(\n",
        "                map(lambda word: self._word2idx.get(word, SkipgramDataset.UNK_TOKEN_IDX), outside_words))\n",
        "            outside_word_indices += [SkipgramDataset.PAD_TOKEN_IDX] * (\n",
        "                    self._window_size * 2 - len(outside_word_indices))\n",
        "\n",
        "            yield center_word_index, torch.Tensor(outside_word_indices).to(torch.long)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koXaJwVdnzoi"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1Krmu4OoPEwnkhaBSsShEMDzEWuMZvVxy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xc7Pq1Mnz5a"
      },
      "source": [
        "\"\"\" Naive softmax loss function for word2vec models\n",
        "\n",
        "Implement the naive softmax losses between a center word's embedding and an outside word's embedding.\n",
        "In addition, using a large batch size reduces the variance of samples in SGD, making training process more effective and accurate.\n",
        "To practice this, let's calculate batch-sized losses of skipgram at once.\n",
        "<PAD> tokens are appended for batching if the number of outside words is less than 2 * window_size. \n",
        "However, these arbitrarily inserted <PAD> tokens have no meaning so should NOT be included in the loss calculation.\n",
        "\n",
        "!!!IMPORTANT: Do NOT forget eliminating the effect of <PAD> tokens!!!\n",
        "\n",
        "Arguments:\n",
        "center_vectors -- center vectors is\n",
        "                  in shape (num words in vocab, word vector length)\n",
        "                  for all words in vocab\n",
        "outside_vectors -- outside vector is\n",
        "                   in shape (num words in vocab, word vector length)\n",
        "                   for all words in vocab\n",
        "center_word_index -- the index of the center word\n",
        "                     in shape (batch size,)\n",
        "outside_word_indices -- the indices of the outside words\n",
        "                        in shape (batch size, window size * 2)\n",
        "\n",
        "Return:\n",
        "losses -- naive softmax loss for each (center_word_index, outsied_word_indices) pair in a batch\n",
        "                        in shape (batch size,)\n",
        "\"\"\"\n",
        "\n",
        "def naive_softmax_loss( center_vectors, outside_vectors,  center_word_index, outside_word_indices):\n",
        "\n",
        "    assert center_word_index.shape[0] == outside_word_indices.shape[0]\n",
        "\n",
        "    # center_vectors.shape = torch.Size([100, 3])\n",
        "    # outside_vectors.shape = torch.Size([100, 3])\n",
        "    # center_word_index.shape = torch.Size([10])\n",
        "    # outside_word_indices.shape = torch.Size([10, 6])\n",
        "\n",
        "    n_tokens, word_dim = center_vectors.shape # ex) 100 * 3\n",
        "    batch_size, outside_word_size = outside_word_indices.shape # ex) 10 * 6\n",
        "    PAD = 0 # PAD_TOKEN_INDEX\n",
        "\n",
        "    ############################################################################\n",
        "    ## TODO : Define loss function\n",
        "    \n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    assert losses.shape == torch.Size([batch_size])\n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-gjmJOEn59p"
      },
      "source": [
        "def test_naive_softmax_loss():\n",
        "    print (\"======Naive Softmax Loss Test Case======\")\n",
        "    \n",
        "    center_word_index = torch.randint(1, 100, [10]) # ex) tensor([39, 60,  9, 63, 28, 39, 90, 80, 48, 54])\n",
        "    outside_word_indices = []\n",
        "    for _ in range(10):\n",
        "        random_window_size = random.randint(3, 6)\n",
        "        outside_word_indices.append([random.randint(1, 99) for _ in range(random_window_size)] + [0] * (6 - random_window_size))\n",
        "    \n",
        "    #outside_word_indices -> ex) [[8, 53, 9, 23, 83, 0], [15, 87, 6, 24, 76, 0], [63, 38, 53, 0, 0, 0], [52, 85, 56, 58, 0, 0], [58, 1, 15, 69, 0, 0], [57, 30, 96, 83, 0, 0], [76, 81, 27, 72, 44, 0], [83, 29, 77, 61, 25, 38], [45, 55, 50, 14, 0, 0], [43, 33, 80, 0, 0, 0]]\n",
        "    outside_word_indices = torch.Tensor(outside_word_indices).to(torch.long)\n",
        "\n",
        "    model = Word2Vec(n_tokens=100, word_dimension=3)\n",
        "\n",
        "    loss = model(center_word_index, outside_word_indices).mean()\n",
        "    loss.backward()\n",
        "\n",
        "    ##################################### First test #####################################\n",
        "    assert (model.center_vectors.grad[0, :] == 0).all() and (model.outside_vectors.grad[0, :] == 0).all(), \\\n",
        "        \"<PAD> token should not affect the result.\"\n",
        "    print(\"The first test passed! Howerver, this test doesn't guarantee you that <PAD> tokens really don't affects result.\")    \n",
        "\n",
        "    ##################################### Second test #####################################\n",
        "    temp = model.center_vectors.grad.clone().detach()\n",
        "    temp[center_word_index] = 0.\n",
        "    assert (temp == 0.).all() and (model.center_vectors.grad[center_word_index] != 0.).all(), \\\n",
        "        \"Only batched center words can affect the center_word embedding.\"\n",
        "    print(\"The second test passed!\")\n",
        "\n",
        "    ##################################### Third test #####################################\n",
        "    assert loss.detach().allclose(torch.tensor(26.86926651)), \\\n",
        "        \"Loss of naive softmax do not match expected result.\"\n",
        "    print(\"The third test passed!\")\n",
        "\n",
        "    ##################################### Fourth test #####################################\n",
        "    expected_grad = torch.Tensor([[-0.07390384, -0.14989397,  0.03736909],\n",
        "                                  [-0.00191219,  0.00386495, -0.00311787],\n",
        "                                  [-0.00470913,  0.00072215,  0.00303244]])\n",
        "    assert model.outside_vectors.grad[1:4, :].allclose(expected_grad), \\\n",
        "        \"Gradients of naive softmax do not match expected result.\"\n",
        "    print(\"The forth test passed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V68CAc91n-OL"
      },
      "source": [
        "torch.set_printoptions(precision=8)\n",
        "torch.manual_seed(4321)\n",
        "random.seed(4321)\n",
        "\n",
        "test_naive_softmax_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M6bT6I4fjv_"
      },
      "source": [
        "### **9) Implement Word2Vec with Negative Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CvUbq7fkKK"
      },
      "source": [
        "class NegSamplingWord2Vec(torch.nn.Module, ABC):\n",
        "    \n",
        "    def __init__(self, n_tokens: int, word_dimension: int, negative_sampler, K: int=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.center_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
        "        self.outside_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self._negative_sampler = negative_sampler\n",
        "        self._K = K\n",
        "\n",
        "    def init_weights(self):\n",
        "        torch.nn.init.normal_(self.center_vectors.data)\n",
        "        torch.nn.init.normal_(self.outside_vectors.data)\n",
        "\n",
        "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
        "        return neg_sampling_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices, self._negative_sampler, self._K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-zHU2IzfqdA"
      },
      "source": [
        "\"\"\" \n",
        "Negative sampling loss function for word2vec models\n",
        "\n",
        "Implement the negative sampling loss for each pair of (center_word_index, outside_word_indices) in a batch.\n",
        "As same with naive_softmax_loss, all inputs are batched with batch_size.\n",
        "\n",
        "Note: Implementing negative sampler is a quite tricky job so we pre-implemented this part. See below comments to check how to use it.\n",
        "If you want to know how the sampler works, check SkipgramDataset.negative_sampler code in dataset.py file\n",
        "\n",
        "Arguments/Return Specifications: same as naiveSoftmaxLoss\n",
        "\n",
        "Additional arguments:\n",
        "negative_sampler -- the negative sampler\n",
        "K -- the number of negative samples to take\n",
        "\"\"\"\n",
        "\n",
        "def neg_sampling_loss(center_vectors, outside_vectors, center_word_index, outside_word_indices, negative_sampler, K: int=10):\n",
        "\n",
        "    assert center_word_index.shape[0] == outside_word_indices.shape[0]\n",
        "\n",
        "    n_tokens, word_dim = center_vectors.shape\n",
        "    batch_size, outside_word_size = outside_word_indices.shape\n",
        "    PAD = SkipgramDataset.PAD_TOKEN_IDX\n",
        "\n",
        "    ##### Sampling negtive indices #####\n",
        "    # Because each outside word needs K negatives samples,\n",
        "    # negative_sampler takes a tensor in shape [batch_size, outside_word_size] and gives a tensor in shape [batch_size, outside_word_size, K]\n",
        "    # where values in last dimension are the indices of sampled negatives for each outside_word.\n",
        "    negative_samples: torch.Tensor = negative_sampler(outside_word_indices, K)\n",
        "    assert negative_samples.shape == torch.Size([batch_size, outside_word_size, K])\n",
        "\n",
        "    batch_center_vectors = center_vectors[center_word_index]\n",
        "    batch_dot_product = torch.einsum('bj,kj->bk', [batch_center_vectors, outside_vectors])\n",
        "\n",
        "    batch_true_loss = torch.log(torch.sigmoid(torch.gather(batch_dot_product, 1, outside_word_indices)))\n",
        "\n",
        "    batch_neg_dots = batch_dot_product.gather(1, negative_samples.reshape(batch_size, outside_word_size * K))\n",
        "    batch_neg_dots = batch_neg_dots.view(batch_size, outside_word_size, K)\n",
        "    batch_neg_loss = torch.sum(torch.log(torch.sigmoid(-batch_neg_dots)), dim=-1)\n",
        "\n",
        "    loss_matrix = -(batch_true_loss + batch_neg_loss)\n",
        "    losses = torch.sum(loss_matrix * (outside_word_indices != 0).int().float(), dim=-1)\n",
        "    \n",
        "    assert losses.shape == torch.Size([batch_size])\n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgKbRamPf6j1"
      },
      "source": [
        "def test_neg_sampling_loss():\n",
        "    print (\"======Negative Sampling Loss Test Case======\")\n",
        "    center_word_index = torch.randint(1, 100, [5])\n",
        "    outside_word_indices = []\n",
        "    for _ in range(5):\n",
        "        random_window_size = random.randint(3, 6)\n",
        "        outside_word_indices.append([random.randint(1, 99) for _ in range(random_window_size)] + [0] * (6 - random_window_size))\n",
        "    outside_word_indices = torch.Tensor(outside_word_indices).to(torch.long)\n",
        "\n",
        "    neg_sampling_prob = torch.ones([100])\n",
        "    neg_sampling_prob[0] = 0.\n",
        "\n",
        "    dummy_database = type('dummy', (), {'_neg_sample_prob': neg_sampling_prob})\n",
        "\n",
        "    sampled_negatives = list()\n",
        "\n",
        "    def negative_sampler_wrapper(outside_word_indices, K):\n",
        "        result = SkipgramDataset.negative_sampler(dummy_database, outside_word_indices, K)\n",
        "        sampled_negatives.clear()\n",
        "        sampled_negatives.append(result)\n",
        "        return result\n",
        "\n",
        "    model = NegSamplingWord2Vec(n_tokens=100, word_dimension=3, negative_sampler=negative_sampler_wrapper, K=5)\n",
        "\n",
        "    loss = model(center_word_index, outside_word_indices).mean()\n",
        "    loss.backward()\n",
        "\n",
        "    ##################################### First test #####################################\n",
        "    assert (model.center_vectors.grad[0, :] == 0).all() and (model.outside_vectors.grad[0, :] == 0).all(), \\\n",
        "        \"<PAD> token should not affect the result.\"\n",
        "    print(\"The first test passed! Howerver, this test dosen't guarantee you that <PAD> tokens really don't affects result.\")    \n",
        "\n",
        "    ##################################### Second test #####################################\n",
        "    temp = model.center_vectors.grad.clone().detach()\n",
        "    temp[center_word_index] = 0.\n",
        "    assert (temp == 0.).all() and (model.center_vectors.grad[center_word_index] != 0.).all(), \\\n",
        "        \"Only batched center words can affect the centerword embedding.\"\n",
        "    print(\"The second test passed!\")\n",
        "\n",
        "    ##################################### Third test #####################################\n",
        "    sampled_negatives = sampled_negatives[0]\n",
        "    sampled_negatives[outside_word_indices.unsqueeze(-1).expand(-1, -1, 5) == 0] = 0\n",
        "    affected_indices = list((set(sampled_negatives.flatten().tolist()) | set(outside_word_indices.flatten().tolist())) - {0})\n",
        "    temp = model.outside_vectors.grad.clone().detach()\n",
        "    temp[affected_indices] = 0.\n",
        "    assert (temp == 0.).all() and (model.outside_vectors.grad[affected_indices] != 0.).all(), \\\n",
        "        \"Only batched outside words and sampled negatives can affect the outside word embedding.\"\n",
        "    print(\"The third test passed!\")\n",
        "\n",
        "    ##################################### Fourth test #####################################\n",
        "    assert loss.detach().allclose(torch.tensor(24.76907349)), \\\n",
        "        \"Loss of negative sampling do not match expected result.\"\n",
        "    print(\"The forth test passed!\")\n",
        "\n",
        "    ##################################### Fifth test #####################################\n",
        "    expected_grad = torch.Tensor([[-0.08732014,  0.02807856, -0.05797456],\n",
        "                                  [ 0.59312224, -0.23522295,  0.10763519],\n",
        "                                  [-0.26151419,  0.37348390, -0.07765102]])\n",
        "\n",
        "    print(model.outside_vectors.grad[affected_indices[:3], :])\n",
        "    assert model.outside_vectors.grad[affected_indices[:3], :].allclose(expected_grad), \\\n",
        "        \"Gradient of negative sampling do not match expected result.\"\n",
        "    print(\"The fifth test passed!\")\n",
        "\n",
        "    print(\"All 5 tests passed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mVJnbQ8f_rV"
      },
      "source": [
        "%%time\n",
        "\n",
        "torch.set_printoptions(precision=8)\n",
        "torch.manual_seed(4321)\n",
        "random.seed(4321)\n",
        "\n",
        "test_neg_sampling_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E173qEAEoDqM"
      },
      "source": [
        "## **2. We will implement SentencePiece and Word2Vec (CBOW)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRkeScp4oHKJ"
      },
      "source": [
        "### **1) Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOS7i2uXoE-L"
      },
      "source": [
        "!pip install sentencepiece\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "!wget https://www.dropbox.com/s/8w9n3cim0b32k2y/train.txt\n",
        "!wget https://www.dropbox.com/s/pwhn9gyjgvg39v5/pos_train.txt\n",
        "!wget https://www.dropbox.com/s/7h8aa1xe270fnfy/neg_train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnibo6XpoLuL"
      },
      "source": [
        "### **2) Import required packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9md2gnIpoL-L"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from gensim.models import word2vec #  word2vec library  \n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews # For Corpus \n",
        "from sklearn.manifold import TSNE # For TSNE\n",
        "from sklearn.metrics import accuracy_score # for calculating accuracy score \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import sentencepiece as spm # For sentencepiece model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZwsU5eDoOmk"
      },
      "source": [
        "### **3) Set Parameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4pgwFPBoQH1"
      },
      "source": [
        "# Parameters for learning skip-gram and cbow model\n",
        "num_features = 300 # Embedding Vector Size\n",
        "negative = 10 # words for negative sampling\n",
        "min_word_count = 10 # minimum words in one sentence\n",
        "window = 5 # context window size\n",
        "downsampling = 0.75 # Lower frequency for high-frequency words \n",
        "epoch = 5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoswo-BEoSPa"
      },
      "source": [
        "### **4) Prepare data & Use SentencePiece**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkcvWc8ToUJr"
      },
      "source": [
        "# preparing data\n",
        "sentences_for_SP = []\n",
        "sentences_naive = []\n",
        "pos_data = open(\"./pos_train.txt\").readlines()\n",
        "neg_data = open(\"./neg_train.txt\").readlines()\n",
        "data_ = pos_data + neg_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmmVc0yOoWk3"
      },
      "source": [
        "for line in data_:\n",
        "    sentences_naive.append(line.strip().split(' '))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvP9yOw5oZlQ"
      },
      "source": [
        "%%time\n",
        "\n",
        "spm.SentencePieceTrainer.train(input=\"./train.txt\", model_prefix='m', vocab_size=10000, user_defined_symbols=['[CLS]', '[SEP]'])\n",
        "sp = spm.SentencePieceProcessor(model_file='./m.model')\n",
        "\n",
        "for line in data_:\n",
        "  sentences_for_SP.append(sp.encode(line, out_type=str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYtZZ2AVoc2o"
      },
      "source": [
        "### **5) Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubolqK50oeG1"
      },
      "source": [
        "%%time\n",
        "\n",
        "# skip-gram model training with naive splitted data\n",
        "naive_model = word2vec.Word2Vec(sentences_naive,\n",
        "                                sg = 1, # CBOW = 0, skip-gram = 1\n",
        "                                negative=negative,\n",
        "                                size=num_features, \n",
        "                                min_count=min_word_count,\n",
        "                                window=window,\n",
        "                                sample=downsampling,\n",
        "                                iter=epoch)\n",
        "\n",
        "# skip-gram model training with sentencepiece data\n",
        "model_with_SP = word2vec.Word2Vec(sentences_for_SP,\n",
        "                                  sg = 1, # CBOW = 0, skip-gram = 1\n",
        "                                  negative=negative,\n",
        "                                  size=num_features, \n",
        "                                  min_count=min_word_count,\n",
        "                                  window=window,\n",
        "                                  sample=downsampling,\n",
        "                                  iter=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk_V8n8gog5p"
      },
      "source": [
        "### **6) Training Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9LSp6QAoiO6"
      },
      "source": [
        "naive_model.wv.most_similar(\"man\") # most similarity word with 'man' in naive skip-gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYtEgpveojsq"
      },
      "source": [
        "vector = naive_model[??] - naive_model[??] + naive_model[??]\n",
        "\n",
        "print(naive_model.wv.similar_by_vector(vector, topn=10, restrict_vocab=None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNZEE0SCEoSM"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1AF-ADZSgqG8NcsO8LgfmF9JXL6F_uQoP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzr7n8jUEolJ"
      },
      "source": [
        "print(naive_model.wv.similarity(w1 = ??, w2 = ??))\n",
        "print(naive_model.wv.similarity(w1 = ??, w2 = ??))\n",
        "print(naive_model.wv.similarity(w1 = ??, w2 = ??))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5_DXbMTomEO"
      },
      "source": [
        "### **7) Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6H9wFhMoosT"
      },
      "source": [
        "def render_TSNE(vocab, word_emb):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        vocab    - vocab list\n",
        "        word_emb - word embeddings\n",
        "    \"\"\"\n",
        "    tsne = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32) # initialization\n",
        "    _tsne = tsne.fit_transform(word_emb) \n",
        "    x_coordinate = _tsne[:,0] # x \n",
        "    y_coordinate = _tsne[:,1] # y\n",
        "\n",
        "    # scatter plot initialization\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(40, 20)\n",
        "    ax.scatter(x_coordinate, y_coordinate)\n",
        "\n",
        "    for i, word in enumerate(random_vocab):\n",
        "        ax.annotate(word,(x_coordinate[i], y_coordinate[i]), fontsize=30) # word labeling for each scatters\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BmWYE9UosJC"
      },
      "source": [
        "vocab = list(naive_model.wv.vocab) # Load vocab list\n",
        "random_vocab = random.sample(vocab,k=100) # Random sampling of 50 words\n",
        "word_emb = naive_model[random_vocab] # Load embedding vector about sampled words\n",
        "render_TSNE(random_vocab, word_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgpC788wgX1c"
      },
      "source": [
        "vocab = list(model_with_SP.wv.vocab) # Load vocab list\n",
        "random_vocab = random.sample(vocab,k=100) # Random sampling of 50 words\n",
        "word_emb = model_with_SP[random_vocab] # Load embedding vector about sampled words\n",
        "render_TSNE(random_vocab, word_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdUY8Eo2otRg"
      },
      "source": [
        "## **HOMEWORK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAlL6YsQowM4"
      },
      "source": [
        "I recommend that you know how to use the tool for future lectures or project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8bZkuweoydo"
      },
      "source": [
        "### **Mosestokenizer && BPE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3-mID-Aougp"
      },
      "source": [
        "############### Reference : http://data.statmt.org/wmt17_systems/training/ ###############\n",
        "############### Reference : https://github.com/rsennrich/subword-nmt.git #################\n",
        "############### Reference : http://www.statmt.org/moses/?n=Moses.Baseline #################\n",
        "# I recommend to see the above reference materials.\n",
        "\n",
        "git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "git clone https://github.com/rsennrich/subword-nmt.git\n",
        "\n",
        "data_dir=/YOUR_DATA_DIRECTORY\n",
        "model_dir=/SAVED_MODEL_DIRECTORY\n",
        "type=ALL or TRAIN \n",
        "moses_scripts=/mosesdecoder/scripts\n",
        "bpe_scripts=/subword-nmt\n",
        "S=de # source language\n",
        "T=en # target language\n",
        "bpe_operations=90000\n",
        "\n",
        "### 1. Tokenizing ###\n",
        "perl $moses_scripts/tokenizer/tokenizer.perl -threads 5 -l $S < $data_dir/$type.$S > $data_dir/$type.tok.$S\n",
        "perl $moses_scripts/tokenizer/tokenizer.perl -threads 5 -l $T < $data_dir/$type.$T > $data_dir/$type.tok.$T\n",
        "\n",
        "\n",
        "### 2. Truecaser ###\n",
        "## Train -- All corpus or specific domain ##\n",
        "perl $moses_scripts/recaser/train-truecaser.perl -corpus $data_dir/$data_dir.tok.$S -model $model_dir/truecase-model.$S\n",
        "perl $moses_scripts/recaser/train-truecaser.perl -corpus $data_dir/$data_dir.tok.$T -model $model_dir/truecase-model.$T\n",
        "\n",
        "## Apply ##\n",
        "perl $moses_scripts/recaser/truecase.perl -model $model_dir/truecase-model.$S < $data_dir/$type.tok.$S > $data_dir/$type.tc.$S\n",
        "perl $moses_scripts/recaser/truecase.perl -model $model_dir/truecase-model.$T < $data_dir/$type.tok.$T > $data_dir/$type.tc.$T\n",
        "\n",
        "### 3. apply bpe --- 'subword-nmt' command ###\n",
        "## Train ##\n",
        "python3 $bpe_scripts/learn_joint_bpe_and_vocab.py -i $data_dir/$type.tc.$S $data_dir/$type.tc.$T --write-vocabulary $data_dir/vocab.$S $data_dir/vocab.$T -s $bpe_operations -o $model_dir/deen.bpe\n",
        "\n",
        "## Apply ##\n",
        "python3 $bpe_scripts/apply_bpe.py -c $model_dir/deen.bpe < $data_dir/$type.tc.$S > $data_dir/$type.bpe.$S\n",
        "python3 $bpe_scripts/apply_bpe.py -c $model_dir/deen.bpe < $data_dir/$type.tc.$T > $data_dir/$type.bpe.$T\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}